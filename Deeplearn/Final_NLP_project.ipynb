{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words as english_words\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "import fasttext\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./kabitakitchen.csv',encoding='Latin-1')\n",
    "\n",
    "df2=pd.read_csv('./Nishafin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop('id',axis=1)\n",
    "df2=df2.drop('id',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=pd.concat([df,df2],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('combined_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_hindi_stops=[\n",
    "    \"aadi\", \"aaj\", \"aap\", \"aapn\", \"aata\", \"aati\", \"aaya\", \"aay\", \"ab\", \"abb\",\n",
    "    \"abbey\", \"abe\", \"abhi\", \"accha\", \"acha\", \"achcha\", \"agar\", \"aint\", \"aisa\", \"ais\",\n",
    "    \"aisi\", \"alag\", \"andar\", \"ap\", \"apan\", \"apna\", \"apnaa\", \"apn\", \"apni\", \"aur\", \"avum\",\n",
    "    \"aya\", \"aye\",\"b\", \"baad\", \"baar\", \"bad\", \"bahut\", \"bana\", \"banai\", \"banao\", \"banaya\",\n",
    "    \"banay\", \"banayi\", \"banda\", \"band\", \"bandi\", \"bane\", \"bani\", \"ba\", \"bata\", \"batao\",\n",
    "    \"bhai\", \"bheetar\", \"bhi\", \"bhitar\", \"bht\", \"bilkul\", \"bohot\", \"bol\", \"bola\", \"bole\",\n",
    "    \"boli\", \"bolo\", \"bolta\", \"bolt\", \"bolti\", \"bro\", \"btw\", \"caus\", \"chahiy\", \"chaiy\",\n",
    "    \"chal\", \"chalega\", \"chhaiy\", \"clearli\", \"com\", \"de\", \"dede\", \"dega\", \"degi\", \"dekh\",\n",
    "    \"dekha\", \"dekh\", \"dekhi\", \"dekho\", \"deng\", \"dhang\", \"di\", \"dijiy\", \"diya\", \"diyaa\",\n",
    "    \"diy\", \"diyo\", \"doosra\", \"doosr\", \"dunga\", \"dungi\", \"dusra\", \"dusr\", \"dusri\", \"dvaara\",\n",
    "    \"dvara\", \"dwaara\", \"dwara\", \"edu\", \"eg\", \"ek\", \"etc\", \"fr\", \"gaya\", \"gay\", \"gayi\",\n",
    "    \"ghar\", \"h\" ,\"haan\",\"hi\" \"hai\", \"hain\", \"hamara\", \"hamar\", \"hamari\", \"hamn\", \"han\", \"har\",\n",
    "    \"hardli\",\"hoga\", \"hoge\", \"hogi\", \"hona\", \"honaa\", \"hone\", \"hong\", \"hongi\", \"honi\",\n",
    "    \"hota\", \"hotaa\", \"hote\", \"hoti\", \"hoyeng\", \"hoyengi\", \"hu\", \"hua\", \"hue\", \"huh\",\n",
    "    \"hui\", \"hum\", \"humein\", \"humn\", \"hun\", \"huy\", \"huyi\", \"i'm\",\"ie\", \"imo\", \"inasmuch\", \"inc\",\n",
    "    \"inh\", \"inhi\", \"inho\", \"inka\", \"inkaa\", \"ink\", \"inki\", \"inn\", \"inner\", \"ins\",\n",
    "    \"insofar\", \"ise\", \"isi\", \"iska\", \"iskaa\", \"isk\", \"iski\", \"ism\", \"isnt\", \"iss\", \"iss\",\n",
    "    \"issi\", \"isski\", \"itna\", \"itn\", \"itni\", \"itno\", \"ityaadi\", \"ityadi\",\"j\", \"ja\", \"jaa\",\n",
    "    \"jab\", \"jabh\", \"jaha\", \"jahaan\", \"jahan\", \"jaisa\", \"jais\", \"jaisi\", \"jata\", \"jayega\",\n",
    "    \"jidhar\", \"jin\", \"jinh\", \"jinhi\", \"jinho\", \"jinhon\", \"jinka\", \"jink\", \"jinki\", \"jinn\",\n",
    "    \"ji\", \"jise\", \"jiska\", \"jisk\", \"jiski\", \"jism\", \"jiss\", \"jiss\", \"jitna\", \"jitn\",\n",
    "    \"jitni\", \"jo\", \"jyaada\", \"jyada\",\"k\",\"ka\", \"kaafi\", \"kab\", \"kabhi\", \"kafi\", \"kaha\",\n",
    "    \"kahaa\", \"kahaan\", \"kahan\", \"kahi\", \"kahin\", \"kaht\", \"kaisa\", \"kais\", \"kaisi\", \"kal\",\n",
    "    \"kam\", \"kar\", \"kara\", \"kare\", \"karega\", \"karegi\", \"karen\", \"kareng\", \"kari\", \"kark\",\n",
    "    \"karna\", \"karn\", \"karni\", \"karo\", \"karta\", \"kart\", \"karti\", \"karu\", \"karun\", \"karunga\",\n",
    "    \"karungi\", \"kaun\", \"kaunsa\", \"kayi\", \"kch\", \"ke\", \"keh\", \"keht\", \"kept\", \"khud\",\n",
    "    \"ki\", \"kin\", \"kine\", \"kinh\", \"kinho\", \"kinka\", \"kink\", \"kinki\", \"kinko\", \"kinn\",\n",
    "    \"kino\", \"ki\", \"kise\", \"kisi\", \"kiska\", \"kisk\", \"kiski\", \"kisko\", \"kisliy\", \"kisn\",\n",
    "    \"kitna\", \"kitn\", \"kitni\", \"kitno\", \"kiya\", \"kiy\", \"ko\", \"koi\", \"kon\", \"konsa\",\n",
    "    \"koyi\", \"krna\", \"krne\", \"kuch\", \"kuchch\", \"kuchh\", \"kul\", \"kull\", \"kya\", \"kyaa\",\n",
    "    \"kyu\", \"kyuki\", \"kyun\", \"kyunki\", \"lagta\", \"lagt\", \"lagti\", \"le\", \"lekar\", \"lekin\",\n",
    "    \"lest\", \"li\", \"liya\", \"liy\", \"lo\", \"log\", \"logon\", \"lol\", \"ltd\", \"lunga\", \"maan\",\n",
    "    \"maana\", \"maan\", \"maani\", \"maano\", \"magar\", \"mai\", \"main\", \"main\", \"mainli\", \"mana\",\n",
    "    \"mane\", \"mani\", \"mano\", \"mani\", \"mat\", \"mein\", \"mera\", \"mere\", \"mere\", \"meri\",\n",
    "    \"mil\", \"mjhe\", \"moreov\", \"mostli\", \"much\", \"mujh\", \"mustnt\",\"n\",\"na\", \"naa\", \"naah\",\n",
    "    \"nahi\", \"nahin\", \"nai\", \"nd\", \"ne\", \"nearli\", \"neech\", \"neednt\", \"nhi\", \"non\",\n",
    "    \"noon\", \"noth\", \"p\",\"par\", \"pata\", \"pe\", \"pehla\", \"pehl\", \"pehli\", \"peopl\", \"per\",\n",
    "    \"phla\", \"phle\", \"phli\", \"plu\",\"plz\", \"poora\", \"poori\", \"pura\", \"puri\", \"que\",\"r\",\"raha\",\n",
    "    \"rahaa\", \"rahe\", \"rahi\", \"rakh\", \"rakha\", \"rakh\", \"rakhen\", \"rakhi\", \"rakho\", \"realli\",\n",
    "    \"reht\", \"rha\", \"rhaa\", \"rhe\", \"rhi\", \"ri\", \"sa\", \"saara\", \"saar\", \"saath\", \"sab\",\n",
    "    \"sabhi\", \"sabs\", \"sahi\", \"sakta\", \"saktaa\", \"sakt\", \"sakti\", \"sang\", \"sara\", \"sath\",\n",
    "    \"secondli\", \"selv\", \"shant\", \"si\", \"sinc\", \"soch\", \"sub\", \"sup\", \"sure\", \"tab\",\n",
    "    \"tabh\", \"tak\", \"tarah\", \"teeno\", \"teesra\", \"teesr\", \"teesri\", \"tend\", \"tera\", \"tere\",\n",
    "    \"teri\",\"theek\", \"thi\", \"tho\", \"thoda\", \"thodi\", \"thru\", \"thu\", \"tjhe\", \"toh\", \"tri\",\n",
    "    \"tri\", \"truli\", \"tri\", \"tu\", \"tujh\", \"tum\", \"tumhara\", \"tumhar\", \"tumhari\", \"tune\",\n",
    "     \"u\",\"um\", \"umm\", \"un\", \"unh\", \"unhi\", \"unho\", \"unhon\", \"unka\", \"unkaa\", \"unk\", \"unki\",\n",
    "    \"unko\", \"unn\", \"uns\", \"upar\", \"us\", \"usi\", \"use\", \"uska\", \"usk\", \"usn\", \"uss\",\n",
    "    \"uss\", \"ussi\", \"vaala\", \"vaal\", \"vaali\", \"vahaan\", \"vahan\", \"vahi\", \"vahin\", \"vaisa\",\n",
    "    \"vais\", \"vaisi\", \"vala\", \"vale\", \"vali\", \"via\", \"viz\", \"vo\", \"waala\", \"waal\", \"waali\",\n",
    "    \"wagaira\", \"wagairah\", \"wagerah\", \"waha\", \"wahaan\", \"wahan\", \"wahi\", \"wahin\", \"waisa\",\n",
    "    \"wais\", \"waisi\", \"wala\", \"wale\", \"wali\", \"wo\", \"woh\", \"wohi\", \"ya\", \"yadi\", \"yah\",\n",
    "    \"yaha\", \"yahaan\", \"yahan\", \"yahi\", \"yahin\", \"ye\", \"yeah\", \"yeh\", \"yehi\", \"ye\", \"yup\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [Errno 11001] getaddrinfo failed>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading words: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "\n",
    "stem=SnowballStemmer('english')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z0-9\\']+')\n",
    "stops=set(stopwords.words('english'))\n",
    "hindi_stops=set(custom_hindi_stops)\n",
    "\n",
    "def preprocess(text):\n",
    "  wordlist=[]\n",
    "\n",
    "  for word in tokenizer.tokenize(text.lower()):\n",
    "      if word not in stops and word not in custom_hindi_stops:\n",
    "        stemmed_word = stem.stem(word)\n",
    "        wordlist.append(stemmed_word)\n",
    "  return wordlist\n",
    "\n",
    "new_df['commentText']=new_df['commentText'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9800, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commentText</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>[superb]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>[ok]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>[mam, tamatu, nibbu, yuz, krte]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>[good]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>[hi, sour, curd, food, curd, normal, sweet, cu...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>[yummi]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>[madam, mujh, aapki, biryani, bnane, trick, ac...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>[tri, process, today, delici, bt, rice, becom,...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>[thing, make, recip, look, easi]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>[ma'am, ora, water, pleas, repli, ma'am]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>[wife, follow, recip, last, night, came, good,...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>[kavita, madam, main, exact, jais, dikhay, lik...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>[yummi]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>[mention, ingredi, descript, box, pleas]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>[today, mom, gave, tri, went, unsuccess, rice,...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>[tri, last, month, yummi, chicken, got, burnt,...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>[excel, present, clean, cook, keep, post, good...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>[delici, kabita, thank, soo]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>[hi, kabita, today, made, chicken, briyani, ur...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>[time, enough, make, chicken, tender, al, dent...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>[marin, oil, add]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>[madam, chicken, mix, karn, tariqa, wrong]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>[hello, kabita, watch, video, video, realli, a...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>[superb, fan, cook]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>[ghee, badl, oil, sakt]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>[nice, chicken, biryani, thank, video, bhejn, ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>[love, cook, style]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>[good, biryani, made, today, style]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>[gem, love]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>[mem, ism, kesar, use, zruri]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>[kabita, mam, follow, regular, ur, dish, nvr, ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>[thank, share, mam]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>[pz, mam, rice, boil, krni, time, batin, 70, p...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>[hungri, kabita]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>[make, everi, dish, simpl, make, fall, love, c...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>[amaz, ma'am, sure, tri, tawa, cook, without, ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>[1, kg, biryani, banay, time, chang, rahega]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>[add, instead, biryani, masala]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>[owowwo, nice, one, mam, madam, 8, parson, chi...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>[thank, recip, wonder, want, add, potato, step...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>[awesom]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>[mam, bataiy, biryani, essenc, rose, water, ke...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>[wow, mam, nice, video, thank]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>[mam, freez, rakhna, jaruri]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>[basmati, rice, use, rose, water, brand]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>[definit, gonna, tri, one, last, time, cook, s...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>[7, 10, oil, nehi, lagana, chahiy, masala, jal...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>[aapn, kons, brand, basmati, rice, hai, ma'am]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>[kabita, ma'am, sch, amaz, cook]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>[mam, 2, chicken, ho, kaisay, bansakhtay, hai,...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           commentText  Labels\n",
       "550                                           [superb]       3\n",
       "551                                               [ok]       6\n",
       "552                    [mam, tamatu, nibbu, yuz, krte]       6\n",
       "553                                             [good]       3\n",
       "554  [hi, sour, curd, food, curd, normal, sweet, cu...       7\n",
       "555                                            [yummi]       2\n",
       "556  [madam, mujh, aapki, biryani, bnane, trick, ac...       6\n",
       "557  [tri, process, today, delici, bt, rice, becom,...       7\n",
       "558                   [thing, make, recip, look, easi]       4\n",
       "559           [ma'am, ora, water, pleas, repli, ma'am]       7\n",
       "560  [wife, follow, recip, last, night, came, good,...       6\n",
       "561  [kavita, madam, main, exact, jais, dikhay, lik...       7\n",
       "562                                            [yummi]       2\n",
       "563           [mention, ingredi, descript, box, pleas]       6\n",
       "564  [today, mom, gave, tri, went, unsuccess, rice,...       6\n",
       "565  [tri, last, month, yummi, chicken, got, burnt,...       7\n",
       "566  [excel, present, clean, cook, keep, post, good...       4\n",
       "567                       [delici, kabita, thank, soo]       5\n",
       "568  [hi, kabita, today, made, chicken, briyani, ur...       5\n",
       "569  [time, enough, make, chicken, tender, al, dent...       7\n",
       "570                                  [marin, oil, add]       7\n",
       "571         [madam, chicken, mix, karn, tariqa, wrong]       6\n",
       "572  [hello, kabita, watch, video, video, realli, a...       4\n",
       "573                                [superb, fan, cook]       4\n",
       "574                            [ghee, badl, oil, sakt]       7\n",
       "575  [nice, chicken, biryani, thank, video, bhejn, ...       5\n",
       "576                                [love, cook, style]       4\n",
       "577                [good, biryani, made, today, style]       2\n",
       "578                                        [gem, love]       4\n",
       "579                      [mem, ism, kesar, use, zruri]       7\n",
       "580  [kabita, mam, follow, regular, ur, dish, nvr, ...       4\n",
       "581                                [thank, share, mam]       1\n",
       "582  [pz, mam, rice, boil, krni, time, batin, 70, p...       7\n",
       "583                                   [hungri, kabita]       6\n",
       "584  [make, everi, dish, simpl, make, fall, love, c...       4\n",
       "585  [amaz, ma'am, sure, tri, tawa, cook, without, ...       7\n",
       "586       [1, kg, biryani, banay, time, chang, rahega]       7\n",
       "587                    [add, instead, biryani, masala]       7\n",
       "588  [owowwo, nice, one, mam, madam, 8, parson, chi...       5\n",
       "589  [thank, recip, wonder, want, add, potato, step...       7\n",
       "590                                           [awesom]       3\n",
       "591  [mam, bataiy, biryani, essenc, rose, water, ke...       7\n",
       "592                     [wow, mam, nice, video, thank]       5\n",
       "593                       [mam, freez, rakhna, jaruri]       7\n",
       "594           [basmati, rice, use, rose, water, brand]       7\n",
       "595  [definit, gonna, tri, one, last, time, cook, s...       6\n",
       "596  [7, 10, oil, nehi, lagana, chahiy, masala, jal...       7\n",
       "597     [aapn, kons, brand, basmati, rice, hai, ma'am]       7\n",
       "598                   [kabita, ma'am, sch, amaz, cook]       4\n",
       "599  [mam, 2, chicken, ho, kaisay, bansakhtay, hai,...       7"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.iloc[550:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=new_df['commentText']\n",
    "y=new_df['Labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [' '.join(document) for document in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl=LabelEncoder()\n",
    "y=lbl.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=to_categorical(y,num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "model_en=fasttext.load_model('./cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04865845, -0.02029513,  0.00281682, ...,  0.04467136,\n",
       "         0.0015281 , -0.00535533],\n",
       "       [ 0.00939113, -0.07598583, -0.01752039, ...,  0.07207707,\n",
       "         0.03232015, -0.03124447],\n",
       "       [ 0.02100777, -0.01061132,  0.00563888, ...,  0.06104375,\n",
       "         0.01299675, -0.00439147],\n",
       "       ...,\n",
       "       [ 0.03667046, -0.00809222, -0.023444  , ...,  0.05898034,\n",
       "         0.01075574, -0.0243484 ],\n",
       "       [ 0.01814937, -0.00052348, -0.05576143, ...,  0.06553596,\n",
       "         0.03396218, -0.02669046],\n",
       "       [ 0.02075222, -0.06449727, -0.03291111, ...,  0.09440843,\n",
       "         0.01172403, -0.03573036]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=np.array([model_en.get_sentence_vector(sentence) for sentence in X])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "53/53 [==============================] - 93s 118ms/step - loss: 1.7215 - accuracy: 0.3940 - val_loss: 1.9313 - val_accuracy: 0.2862\n",
      "Epoch 2/150\n",
      "53/53 [==============================] - 1s 15ms/step - loss: 1.2456 - accuracy: 0.5811 - val_loss: 1.9590 - val_accuracy: 0.1679\n",
      "Epoch 3/150\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 1.0638 - accuracy: 0.6478 - val_loss: 1.9344 - val_accuracy: 0.1714\n",
      "Epoch 4/150\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.9730 - accuracy: 0.6818 - val_loss: 1.9280 - val_accuracy: 0.1918\n",
      "Epoch 5/150\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.9283 - accuracy: 0.6960 - val_loss: 1.7568 - val_accuracy: 0.2189\n",
      "Epoch 6/150\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.8879 - accuracy: 0.7165 - val_loss: 1.7175 - val_accuracy: 0.3357\n",
      "Epoch 7/150\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.8667 - accuracy: 0.7193 - val_loss: 1.4461 - val_accuracy: 0.4260\n",
      "Epoch 8/150\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.8342 - accuracy: 0.7321 - val_loss: 1.3621 - val_accuracy: 0.4827\n",
      "Epoch 9/150\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.8001 - accuracy: 0.7462 - val_loss: 1.1031 - val_accuracy: 0.6270\n",
      "Epoch 10/150\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.7941 - accuracy: 0.7439 - val_loss: 0.9384 - val_accuracy: 0.6903\n",
      "Epoch 11/150\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.7657 - accuracy: 0.7568 - val_loss: 0.8805 - val_accuracy: 0.7173\n",
      "Epoch 12/150\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.7616 - accuracy: 0.7584 - val_loss: 0.8446 - val_accuracy: 0.7296\n",
      "Epoch 13/150\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.7503 - accuracy: 0.7602 - val_loss: 0.7976 - val_accuracy: 0.7413\n",
      "Epoch 14/150\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.7460 - accuracy: 0.7634 - val_loss: 0.7885 - val_accuracy: 0.7439\n",
      "Epoch 15/150\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.7274 - accuracy: 0.7662 - val_loss: 0.7902 - val_accuracy: 0.7444\n",
      "Epoch 16/150\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.7188 - accuracy: 0.7666 - val_loss: 0.7748 - val_accuracy: 0.7541\n",
      "Epoch 17/150\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.7045 - accuracy: 0.7712 - val_loss: 0.7741 - val_accuracy: 0.7510\n",
      "Epoch 18/150\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.7040 - accuracy: 0.7713 - val_loss: 0.7767 - val_accuracy: 0.7510\n",
      "Epoch 19/150\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.6891 - accuracy: 0.7841 - val_loss: 0.7711 - val_accuracy: 0.7439\n",
      "Epoch 20/150\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6809 - accuracy: 0.7776 - val_loss: 0.7754 - val_accuracy: 0.7510\n",
      "Epoch 21/150\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6828 - accuracy: 0.7772 - val_loss: 0.7792 - val_accuracy: 0.7449\n",
      "Epoch 22/150\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.6711 - accuracy: 0.7858 - val_loss: 0.7803 - val_accuracy: 0.7480\n",
      "Epoch 23/150\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.6728 - accuracy: 0.7828 - val_loss: 0.7949 - val_accuracy: 0.7480\n",
      "Epoch 24/150\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.6757 - accuracy: 0.7784 - val_loss: 0.7977 - val_accuracy: 0.7398\n",
      "Epoch 25/150\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.6576 - accuracy: 0.7867 - val_loss: 0.7804 - val_accuracy: 0.7418\n",
      "Epoch 26/150\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6626 - accuracy: 0.7847 - val_loss: 0.7871 - val_accuracy: 0.7434\n",
      "Epoch 27/150\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6494 - accuracy: 0.7872 - val_loss: 0.7617 - val_accuracy: 0.7526\n",
      "Epoch 28/150\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6414 - accuracy: 0.7955 - val_loss: 0.7851 - val_accuracy: 0.7444\n",
      "Epoch 29/150\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6341 - accuracy: 0.7978 - val_loss: 0.7745 - val_accuracy: 0.7510\n",
      "Epoch 30/150\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6367 - accuracy: 0.7985 - val_loss: 0.7740 - val_accuracy: 0.7469\n",
      "Epoch 31/150\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6341 - accuracy: 0.7922 - val_loss: 0.7852 - val_accuracy: 0.7485\n",
      "Epoch 32/150\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6285 - accuracy: 0.7977 - val_loss: 0.7780 - val_accuracy: 0.7474\n",
      "Epoch 33/150\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6302 - accuracy: 0.7964 - val_loss: 0.7720 - val_accuracy: 0.7383\n",
      "Epoch 34/150\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6229 - accuracy: 0.7999 - val_loss: 0.7757 - val_accuracy: 0.7454\n",
      "Epoch 35/150\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.6192 - accuracy: 0.8000 - val_loss: 0.7750 - val_accuracy: 0.7485\n",
      "Epoch 36/150\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6087 - accuracy: 0.7982 - val_loss: 0.7901 - val_accuracy: 0.7413\n",
      "Epoch 37/150\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6178 - accuracy: 0.7985 - val_loss: 0.7835 - val_accuracy: 0.7429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x199eaf004d0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, PReLU,LeakyReLU\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.regularizers import l1, l2\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# Reshape input data for LSTM\n",
    "X_train = X_train.reshape(-1,1,300)\n",
    "X_test = X_test.reshape(-1,1,300)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(64, input_shape=(1,300)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(25,activation=PReLU(),kernel_regularizer=l2(0.001)))\n",
    "\n",
    "model.add(Dense(40,activation=LeakyReLU(),kernel_regularizer=l2(0.001)))\n",
    "\n",
    "model.add(Dense(30,activation='relu',kernel_regularizer=l2(0.001)))\n",
    "\n",
    "model.add(Dense(35,activation='tanh',kernel_regularizer=l2(0.001)))\n",
    "\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# Train the model on this fold\n",
    "model.fit(X_train, Y_train, epochs=150, batch_size=150, validation_data=(X_test, Y_test), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 1s 2ms/step - loss: 0.5523 - accuracy: 0.8310\n",
      "Accuracy on train data: 83.10%\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.evaluate(X_train, Y_train)[1]\n",
    "print(\"Accuracy on train data: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 0s 4ms/step - loss: 0.7617 - accuracy: 0.7526\n",
      "Accuracy on test data: 75.26%\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.evaluate(X_test, Y_test)[1]\n",
    "print(\"Accuracy on test data: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
